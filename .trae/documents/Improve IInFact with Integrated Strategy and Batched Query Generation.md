我将修改 `IInFact`，通过将“解读”和“查询生成”步骤集成到一个智能的初始 Prompt 中，从而在严格控制成本的同时提升效果。

### 1. 改进解读步骤：引入“验证策略” (Verification Strategy)
- **问题**：原有的“解读”步骤较为模糊，且需要一次独立的 LLM 调用，性价比低。
- **改进方案**：我将在生成问题前，要求模型先输出一段**“验证策略”**。这要求模型先分析主张中的关键实体和需要验证的核心关系，以此作为“思维链”来指导后续提问。
- **优势**：这在**不增加任何额外 API 调用**的情况下，通过显式的推理步骤提升了问题的针对性和质量。

### 2. 改进查询生成：上下文感知的批量生成 (Contextual Batching)
- **问题**：原版为每个问题单独生成查询（`ProposeQueries`）会导致 $N$ 次额外的 LLM 调用，成本极高且缺乏上下文；而 `NoQueryGeneration` 虽然便宜但不够精准。
- **改进方案**：在初始生成问题时，要求模型**同时为每个问题生成一个优化后的搜索查询**。
- **优势**：
    - **成本控制**：将原本的 $N+1$ 次调用（1次提问 + 10次生成查询）减少为**仅 1 次调用**。
    - **效果提升**：模型在拥有完整主张上下文的情况下生成的查询，比后续孤立生成的查询更准确，更能捕捉搜索意图。

### 具体实现计划
我将修改 `src/infact/procedure/variants/qa_based/iinfact.py`：
1.  **重写 `apply_to`**：在流程开始时初始化一个查询缓存 (`_query_cache`)。
2.  **重写 `_pose_questions`**：
    - 使用自定义 Prompt，要求模型输出：
        1.  验证策略（文本分析）。
        2.  成对的“自然语言问题”与“搜索引擎查询”（Markdown 代码块格式）。
    - 解析输出，将“查询”存入缓存，返回“问题”列表给主流程。
3.  **重写 `propose_queries_for_question`**：
    - 不再调用 LLM，而是直接从 `_query_cache` 中读取预生成的查询。
    - 如果未命中缓存，则回退到使用原始问题（保障鲁棒性）。

这个方案完美契合了您的要求：既“改进了效果不好部分”（通过策略分析和上下文查询生成），又“注意了成本控制”（大幅减少 API 调用次数）。
